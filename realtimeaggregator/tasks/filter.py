"""Provides the filter task object."""

import glob
import os
import time
import shutil
from .common import settings
from .common import task
from .common import log_templates_pb2
from . import tools


class FilterTask(task.Task):
    """This class provides the mechanism for performing filter tasks.

    The filter task filters downloads in the sense that it looks for duplicate
    feed downloads and removes them. If following a strategy of downloading
    feeds at a fixed interval of time, as the download task does, it is likely
    that there will be many duplicates, especially if downloading frequently
    for redundancy.

    The filter task iterates through each file in the downloaded directory.
    For each file it tries to interpret it (given the file name) as a feed
    download, if it can't the file is deleted. For valid downloads, the task
    attempts to read the file using the user provided timestamp function.
    The timestamp function takes the path of the file and returns the time
    the feed was generated by the transit authority. If the function returns
    -1, or if an exception is raised, that download is deemed corrupt and
    deleted. Otherwise the file is copied to the filtered directory, with a
    new file name based on the transit authority timestamp. Duplicate files
    are easily discovered as the transit authority timestamp, and hence the
    new file name, are the same.

    To use the download task, initialize in the common way for all tasks:

        task = FilterTask(root_dir=, feeds=, quiet=, log_file_path=)

    see the task class for details on the arguments here. Additional
    initialization is likely desired by setting the limit attribute:

        task.limit = 1000

    The task is then run using the run() method:

        task.run()

    Attributes:
        limit (int): an integer imposing an upper bound on the number of
                     downloaded files to process. If set to -1, no limit
                     is imposed. Default is -1.
        n_total (int): number of downloaded files processed.
        n_corrupt (int): number of downloaded files that were deemed corrupt.
        n_copied (int): number of downloaded files copied to the filtered
                        directory.
        n_skipped (int): number of downloaded files that were not copied
                         because they were duplicated of files already copied.
    """

    def __init__(self, **kwargs):

        super().__init__(**kwargs)
        # Initialize the log directory
        self._init_log('filter', log_templates_pb2.FilterTaskLog)

        # Initialize task configuration
        self.limit = -1
        self.file_access_lag = 120



    def _run(self):
        """Run a filter task."""

        # Place the task configuration in the log
        self._log_run_configuration()
        self._log.limit = self.limit
        self._log.file_access_lag = self.file_access_lag

        # Initialize some necessary variables
        feeds_by_id = {}
        for feed in self.feeds:
            feeds_by_id[feed[0]] = feed

        self.num_corrupt_by_feed_id = {feed_id: 0 for feed_id in feeds_by_id}
        self.num_copied_by_feed_id = {}
        self.num_duplicate_by_feed_id = {}
        for feed in self.feeds:
            feeds_by_id[feed[0]] = feed
            self.num_corrupt_by_feed_id[feed[0]] = 0
            self.num_copied_by_feed_id[feed[0]] = 0
            self.num_duplicate_by_feed_id[feed[0]] = 0

        self.num_total = 0
        self.num_zombie = 0
        last_download_time = -1
        limit_reached = False
        self.source_dirs = set()
        self.target_dirs = set()

        # Walk over the directory containing the raw downloaded files,
        # inspecting every file. We attempt to process files that are
        # feed downloaded, and remove other ('zombie') files.
        downloaded_dir = os.path.join(
            self._storage_dir,
            'feeds',
            'downloaded'
            )
        filtered_dir = os.path.join(
            self._storage_dir,
            'feeds',
            'filtered'
            )

        for sub_dir, _, files in os.walk(downloaded_dir):
            for source_file_name in files:
                source_file_path = os.path.join(sub_dir, source_file_name)
                # Attempt to interpret the file as a feed download,
                # whose filename has the form
                # [UID]-YYYY-MM-DDTHHMMSSZ-dt.[EXT]
                # First, extract the UID from the file name and see if
                # such a UID exists.
                # Second, ensure the file extension matches that of
                # the UID.
                # Finally, check the UTC string is valid (and if so
                # store it: this is the download time).
                # If it's not a valid file, delete it.
                try:
                    i = source_file_name.find('-')
                    feed_id = source_file_name[:i]
                    feed_download_time = source_file_name[i+1:i+19]
                    feed_ext = source_file_name[i+23:]
                    cond1 = (feed_id not in feeds_by_id)
                    cond2 = (feed_ext != feeds_by_id[feed_id][2])
                    if cond1 or cond2:
                        raise Exception(
                            'UID and/or EXT does not match feeds.'
                            )
                    downloaded_timestamp = (
                        tools.time.utc_8601_to_timestamp(feed_download_time)
                        )
                    if downloaded_timestamp > last_download_time:
                        last_download_time = downloaded_timestamp
                except Exception as e:
                    self.num_zombie += 1
                    os.remove(file_path)
                    self._print('Deleted zombie file: ' + source_file_path)
                    continue

                # Check the last modification time; if it was within
                # file_access_lag seconds ignore this file to avoid
                # file I/O clash with a download task
                if(self.time()-os.path.getmtime(source_file_path)
                   < self.file_access_lag):
                    continue

                # This file will be processed, so increment the
                # n_total counter
                self.num_total += 1
                self.source_dirs.add(sub_dir)

                # The next step is to try to read the feed timestamp: the
                # time the transit authority distributed the feed.
                # In general this will be different to the download time
                # because we may have downloaded a few seconds late.
                # The timestamp is read using the user-provided function.
                # If there is an exception raised, or if the timestamp is
                # negative, mark the file as corrupt and delete it.
                try:
                    feed_timestamp = feeds_by_id[feed_id][3](source_file_path)
                    if feed_timestamp < 0:
                        raise Exception('File timestamp is negative.')
                except Exception as e:
                    self._print('Corrupt file: {}'.format(source_file_path))
                    self._print('  (reason given: {})'.format(str(e)))
                    self.num_corrupt_by_feed_id[feed_id] += 1
                    os.remove(source_file_path)
                    continue

                # The file is valid, we now move it over the filtered
                # directory.
                # First we calculate the target directory and target
                # filename within the filtered store using the timestamp
                (day_dir, hour_sub_dir, file_time) = (
                    tools.time.timestamp_to_path_pieces(self.time())
                    )
                target_dir = os.path.join(
                    filtered_dir,
                    day_dir,
                    hour_sub_dir,
                    feed_id
                    )
                target_file_name = '{}-{}.{}'.format(
                        feed_id,
                        tools.time.timestamp_to_utc_8601(feed_timestamp),
                        feed_ext
                        )
                target_file_path = os.path.join(
                    target_dir,
                    target_file_name
                    )
                tools.filesys.ensure_dir(target_dir)

                # If the target file does not exist, copy it
                # Otherwise this is a duplicate feed and can be skipped
                if not os.path.isfile(target_file_path):
                    shutil.copyfile(source_file_path, target_file_path)
                    self.num_copied_by_feed_id[feed_id] += 1
                    self.target_dirs.add(target_dir)
                else:
                    self.num_duplicate_by_feed_id[feed_id] += 1

                # Remove the original downloaded file
                os.remove(source_file_path)

                # Check if the limit of number of files to be processed
                # has been reached
                if self.limit >= 0 and self.num_total >= self.limit:
                    limit_reached = True
                    self._log.limit_reached = True
                    self._print('Copy threshold reached; closing.')
                    self._print('Run again to filter more files.')
                    break

            # We're in a double for loop here, so need to break out
            # of the second one too
            if limit_reached:
                break

        # At this stage the filtering process has ended, and we need to do
        # some cleaning up.
        # First, by moving downloaded files over we may have a lot of empty
        # subdirectories in the downloaded store, delete these.
        tools.filesys.prune_directory_tree(downloaded_dir)

        # We need to record the last downloaded time; this is needed to
        # determine when all downloads for a given clock hour
        # are done and can be compressed.
        time_tracker = tools.latesttimetracker.LatestTimeTracker(
            os.path.join(filtered_dir, 'last_download_time')
            )
        latest_time = time_tracker.add_time(last_download_time)

        # If the limit was not reached, so that all downloads were filtered,
        # we may have to schedule compressions
        if limit_reached is False:
            # Iterate over all the filtered directories
            # If the corresponding hour is before the timestamp, then
            # schedule for compression.
            # The scheduling is done by setting the compress flag, which
            # is an empty file in the hour's directory entitled 'compress'
            files = glob.glob(
                filtered_dir +
                '[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]/[0-9][0-9]'
                )
            for file_path in files:
                hour = file_path[-2:]
                date = file_path[-13:-3]
                utc = date + 'T' + hour + '5959Z'
                t = tools.time.utc_8601_to_timestamp(utc)
                if t < latest_time:
                    tools.filesys.touch(file_path + '/compress')
                    self._log.scheduled_compressions.append(t)
                    #self.log.write(
                    #        'Hour {}T{} '.format(date, hour) +
                    #        'scheduled for compression.')


        # Write the concluding statistics to the log file
        for feed_id in feeds_by_id:
            self._log.num_copied.append(self.num_copied_by_feed_id[feed_id])
            self._log.num_duplicate.append(self.num_duplicate_by_feed_id[feed_id])
            self._log.num_corrupt.append(self.num_corrupt_by_feed_id[feed_id])
        for source_dir in self.source_dirs:
            self._log.source_directories.append(source_dir)
        for target_dir in self.target_dirs:
            self._log.target_directories.append(target_dir)

        # Write some user friendly statistics to the terminal
        total_corrupt = sum(self.num_corrupt_by_feed_id.values())
        total_copied = sum(self.num_copied_by_feed_id.values())
        total_duplicate = sum(self.num_duplicate_by_feed_id.values()) 
        if self.num_total > 0 :
            percent_valid = 100 * (total_duplicate + total_copied) / (self.num_total)
        else:
            percent_valid = 100
        if total_duplicate + total_copied > 0:
            percent_duplicate = 100 * total_duplicate/(total_duplicate + total_copied)
        else:
            percent_duplicate = 100
        self._print('Filter task ended. Statistics:')
        self._print('  * {} zombie files (deleted).'.format(self.num_zombie))
        self._print('  * {} feed downloads processed.'.format(self.num_total))
        self._print('  * {} corrupt downloads.'.format(total_corrupt))
        self._print('  * {} valid downloads.'.format(total_copied+total_duplicate))
        self._print('  * {:2}% of downloads deemed valid.'.format(percent_valid))
        self._print('  * {:2}% of downloads were duplicates.'.format(percent_duplicate))
    
